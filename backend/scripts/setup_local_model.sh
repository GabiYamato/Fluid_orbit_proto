#!/bin/bash

echo "üöÄ Setting up Local Llama (Meta) Model via Ollama..."

# 1. Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "üì¶ Installing Ollama..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        if command -v brew &> /dev/null; then
            echo "üç∫ Using Homebrew to install Ollama..."
            brew install ollama
        else
            echo "‚ùå Homebrew not found. Please download Ollama from https://ollama.com/download/mac"
            exit 1
        fi
    else
        # Linux
        curl -fsSL https://ollama.com/install.sh | sh
    fi
else
    echo "‚úÖ Ollama is already installed."
fi

# 2. Start Ollama in background (unless running)
if ! pgrep -x "ollama" > /dev/null; then
    echo "üîÑ Starting Ollama server..."
    # On mac, brew services might be better, but 'ollama serve' works
    ollama serve &
    OLLAMA_PID=$!
    echo "‚è≥ Waiting 10s for Ollama to initialize..."
    sleep 10
else
    echo "‚úÖ Ollama server is running."
fi

# 3. Pull models
# llama3.2:3b is faster. nomic-embed-text for embeddings.
MODEL_NAME="llama3.2:3b"
EMBEDDING_MODEL="nomic-embed-text"

echo "‚¨áÔ∏è  Pulling $MODEL_NAME..."
ollama pull $MODEL_NAME
echo "‚¨áÔ∏è  Pulling $EMBEDDING_MODEL..."
ollama pull $EMBEDDING_MODEL

echo "‚ú® Model ready!"
echo "üß™ Running a test prompt..."

ollama run $MODEL_NAME "Hello! Explain what RAG is in one sentence."

echo ""
echo "üéâ Setup Complete!"
echo "To use this in ShopGPT:"
echo "1. We will update RAGService to point to localhost:11434"
